{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d22d2c",
   "metadata": {},
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from numpy import savetxt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "device=torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5835674d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import dataloader, Dataset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from numpy import savetxt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ef7f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main\\data/train.tsv',header = 0, delimiter = '\\t')\n",
    "df_test = pd.read_csv('D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main\\data/test.tsv',header = 0, delimiter = '\\t')\n",
    "df_train.info()  #对pandas读取的数据给出一个summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a712bcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()  #返回读取数据的前五行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ede1394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.509945\n",
       "3    0.210989\n",
       "1    0.174760\n",
       "4    0.058990\n",
       "0    0.045316\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Sentiment.value_counts()/df_train.Sentiment.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c470d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.Phrase.str.len().max()   #找数据中最长的一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcec4b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17694b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16256/1651122775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Phrase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mceshi_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Phrase'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#转化为list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "X = df_train['Phrase']\n",
    "Y = df_train['Sentiment']\n",
    "ceshi_data=df_test['Phrase']\n",
    "all_data=list(X) #转化为list\n",
    "all_labels=list(Y)\n",
    "c=list(zip(all_data,all_labels)) #data和label一对一压起来\n",
    "random.shuffle(c)\n",
    "all_data[:],all_labels[:]=zip(*c)#解压\n",
    "length_train=int(len(all_data)*0.8)\n",
    "train_data=all_data[:length_train]\n",
    "train_labels=all_labels[:length_train]\n",
    "test_data=all_data[length_train:]\n",
    "test_labels=all_labels[length_train:]\n",
    "len(train_data),len(train_labels),len(test_data),len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b3a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentiment(data):\n",
    "    def tokenizer(text):\n",
    "        return[tok.lower() for tok in text.split(' ')]   #转换大小写，分割text\n",
    "    return[tokenizer(review) for review in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc43dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#words in vocab:', 16532)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_sentiment(data):\n",
    "    tokenized_data=get_tokenized_sentiment(data)\n",
    "    counter=collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "\n",
    "    return Vocab.vocab(counter)   #排除出现五次以下的sentiment，返回一个字典    删除 min_freq=5\n",
    "vocab=get_vocab_sentiment(all_data)\n",
    "'#words in vocab:',len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e4b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentiment(data,vocab,labels=None):\n",
    "    max_l=250\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    tokenized_data=get_tokenized_sentiment(data)\n",
    "    features=torch.tensor([pad([vocab.__getitem__(word)for word in words])for words in tokenized_data])  #stoi()–>将字符串转换为int, 然后pad每一条评论\n",
    "    if labels:\n",
    "        labels=torch.tensor(labels)    #????????????\n",
    "        return features, labels\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "155fd34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15724/968907959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpreprocess_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpreprocess_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "train_set=TensorDataset(*preprocess_sentiment(train_data,vocab,train_labels))\n",
    "test_set=TensorDataset(*preprocess_sentiment(test_data,vocab,test_labels))\n",
    "train_iteration=dataloader(train_set,batch_size,shuffle=True)\n",
    "test_iteration=dataloader(test_set,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c279f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a33838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab,embed_size,num_hidden,num_layers):\n",
    "        super(BiRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.encoder=nn.LSTM(input_size=embed_size,hidden_size=num_hidden,num_layers=num_layers,bidirectional=True)\n",
    "        #双向，最终步和最初步，所以是4*num_hiddens\n",
    "        self.decoder=nn.Linear(4*num_hidden,5)  #输入维度4*num，输出维度为5，训练一个线性函数来拟合\n",
    "    def forward(self,inputs):\n",
    "        embedding=self.embedding(inputs.permute(1,0)) #permute 交换tensor维度\n",
    "        outputs,_=self.encoder(embedding)#输出(h,c)\n",
    "        encoding=torch.cat(outputs(0),output(-1),-1) #按最后一维拼接\n",
    "        out=self.decoder(encoding)\n",
    "        return outs\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714c2b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20904/1339913171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_nums\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet_withpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBiRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithoutpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBiRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "embed_size,hidden_nums,hidden_layers=100,100,2\n",
    "net_withpretrained=BiRNN(vocab,embed_size,num_hidden,hum_layers)\n",
    "net.withoutpretrained=BiRNN(vocab,embed_size,num_hidden,hum_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb2eb7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main/data/glove\\glove.6B.zip: 862MB [09:49, 1.46MB/s]          \n",
      "100%|██████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:19<00:00, 20237.13it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_vocab=Vocab.GloVe(name='6B',dim=100,cache='D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main/data/glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05437465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(words,pretrain_vocab):\n",
    "    embed=torch.zeros(len(words),pretrain_vocab.vectors[0].shape[0])\n",
    "    oov_count=0\n",
    "    for i,word in enumerate(words):   #enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列\n",
    "        \n",
    "        #异常检测   \n",
    "        try:\n",
    "            idx=pretrain_vocab.get_stoi(word) #stoi   现版本用法\n",
    "            embed[i,:]=pretrain_vocab.vectors[idx]\n",
    "        except KeyError:  #KeyError 映射中没有这个键，此处为无对应的词向量\n",
    "            oov_count+=1\n",
    "        if oov_count>0:\n",
    "            print(\"There is %d OOV words\" %oov_count)\n",
    "            return embed\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_withpretrained.embedding.weight.data.copy_(vocab.itos,glove_vocab)#tensor.copy_ 从vocab复制数据\n",
    "# nn.Embedding具有一个权重（.weight），形状是(num_words, embedding_dim)\n",
    "net_withpretrained.embedding.weight.requires_grad=False\n",
    "#requires_grad=True 则为这个tensor计算梯度    已经是预训练好的词向量，这里就不再需要进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter,net,device=None):\n",
    "    if device is None and isinstsance(net,torch.nn.Module):    #isinstance() 函数来判断一个对象是否是一个已知的类型\n",
    "        device=list(net.parameter())[0].device\n",
    "        acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,nn.Module):\n",
    "                net.eval()# 评估模式, 这会关闭dropout\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()# 改回训练模式\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    # items()的作用是把字典中的每对key和value组成一个元组，并把这些元祖放在列表中返回，item()函数取出的元素值的精度更高，所以在求损失函数等时我们一般用item（）\n",
    "                    #argmax返回指定维度最大值的序号\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() #acc sum了什么\n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n+=y.shape[0]\n",
    "        return acc_sum/n\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "202334b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter,test_iter,net, loss,optimizer,device,num_epochs):\n",
    "    #model.to(cuda)   copy model to cuda (or cpu) 将所有最开始读取数据时的tensor变量copy一份到device所指定的GPU上去，之后的运算都在GPU上进行\n",
    "    #equveliant to model.cuda()\n",
    "    #torch.device代表将torch.Tensor分配到的设备的对象。\n",
    "    #torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。\n",
    "        net=net.to(device)\n",
    "        print('training on',device)\n",
    "        batch_count=0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_1_sum,train_acc_sum,n,start=0.0,0.0,0,time.time()\n",
    "            for X,y in train_iter:\n",
    "                X=X.to(device)\n",
    "                y=y.to(device)\n",
    "                y_hat=net(X)\n",
    "                l=loss(y_hat,y)\n",
    "                #清零梯度，反向传播，计算梯度\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                train_1_sum+=cpu().sum().item() #sum loss of each batch\n",
    "                # argmax返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同\n",
    "                # 相等条件判断式(y_hat.argmax(dim=1) == y)是一个类型为ByteTensor的Tensor\n",
    "                # 判断式结果为值为0（相等为假）或1（相等为真）的Tensor，然后sum求和，得到判断正确的次数，再求平均得到正确率\n",
    "                train_acc_sum=+=(y_hat.argmax(dim=1)==y).sum().cpu().item()\n",
    "                n+=yshape(0)\n",
    "                batch_count+=1\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print('epochs %d: loss %4f: train accuracy %3f: test accuracy %3f: time %f: sec'%\n",
    "               epochs+1,train_1_sum,train_acc_sum/n,test_acc,time.time()-start)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "698c4b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net_withpretrained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20904/3897227075.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#filter(lambda p: p.requires_grad, model.parameters())中 lambda p: p.requires_grad就是以p为参数的满足p.requires_grad的true的条件的函数。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#而参数p赋值的元素从列表model.parameters()里取。所以只取param.requires_grad = True（模型参数的可导性是true的元素），就过滤掉为false的元素。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet_withpretrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevicenum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net_withpretrained' is not defined"
     ]
    }
   ],
   "source": [
    "lr,num_epochs=0.01,5\n",
    "#lambda arg : expression，解释：冒号前是参数，冒号后是返回值。\n",
    "#filter根据判断结果自动过滤掉不符合条件的元素，并返回有符合元素组成的新列表\n",
    "#filter(lambda p: p.requires_grad, model.parameters())中 lambda p: p.requires_grad就是以p为参数的满足p.requires_grad的true的条件的函数。\n",
    "#而参数p赋值的元素从列表model.parameters()里取。所以只取param.requires_grad = True（模型参数的可导性是true的元素），就过滤掉为false的元素。\n",
    "optimizer=torch.optim.Adam(filter(lambda p:p.requires_grad,net_withpretrained.parameters()),lr=lr)\n",
    "loss=bb.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,net,loss,optimier,devicenum_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs=0.01,5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "train(train_iter,test_iter,net_witoutpretrained,loss,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4131ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ceshi_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20904/2054362878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mceshi_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mceshi_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mpredict_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mceshi_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnet_withpretrained\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ceshi_set' is not defined"
     ]
    }
   ],
   "source": [
    "def test_predict(data_iter,net,device):\n",
    "    net=net.to(device)\n",
    "    print('training on',device)\n",
    "    predict=[]\n",
    "    i=0\n",
    "    start=time.time()\n",
    "    for X in data_iter:\n",
    "        X=X.to(device)\n",
    "        y=net(X.view(1,-1).argmax(dim=1))\n",
    "        predict.append(y.cpu().item())\n",
    "        i+=1\n",
    "        if (i%5000==0):\n",
    "            print('%d samples predicted,%d samples left, cost time %f'% (i,(ceshi_num-i),time.time()-star))\n",
    "            start=time.time()\n",
    "        return predict\n",
    "ceshi_iter=iter(ceshi_set)\n",
    "predict_list=test_predict(ceshi_iter,net_withpretrained,device)\n",
    "                  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2180860f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20904/4209429965.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m156061\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main\\sentiment-analysis-on-movie-reviews/rnn_benchmark.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdilimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%d,%d'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'PhraseID,Sentiment'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_list' is not defined"
     ]
    }
   ],
   "source": [
    "pred=[[index+156061,x]for index,x in enumerate(predict_list)]\n",
    "savetxt('D:\\计算机\\代码\\FudanNLP_Begginer-main\\FudanNLP_Begginer-main\\sentiment-analysis-on-movie-reviews/rnn_benchmark.csv',pred,dilimiter=',',fmt='%d,%d',header='PhraseID,Sentiment',comment='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc48cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20904/781230502.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mb\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "a=torch.randn([1,3]) \n",
    "b=0 \n",
    "for i in range(3): \n",
    "    b+=(torch.randn([1,3]).argmax(dim=0)==a[i]).float().sum().item() \n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04e1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
